{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## exp2 - Gemma with competition dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modal\n",
    "# !pip install bitsandbytes==0.42.0 peft==0.11.0 -qq\n",
    "# !pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jarvis = False\n",
    "modal = True\n",
    "vastai = False\n",
    "hf = False\n",
    "OUTPUT_DIR = \"output_exp2\"\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json \n",
    "import neptune\n",
    "\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel, Gemma2PreTrainedModel,\n",
    "    LlamaModel,Gemma2Model,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if jarvis:\n",
    "    NEPTUNE_API_TOKEN = json.load(open(\"../apikeys.json\"))[\"NEPTUNE_API_TOKEN\"]\n",
    "    TRAIN_CSV = \"/home/lmsys/input/train.csv\"\n",
    "    FOLD_PATH = \"//home/lmsys/input/fold.pkl\"\n",
    "elif modal:\n",
    "    NEPTUNE_API_TOKEN = json.load(open(\"apikeys.json\"))[\"NEPTUNE_API_TOKEN\"]\n",
    "    TRAIN_CSV = \"/root/cache/input/train.csv\"\n",
    "    FOLD_PATH = \"/root/cache/input/fold.pkl\"\n",
    "    EXTERNAL_CSV = \"/root/cache/input/external_datasets/lmsys_ext_df_hs.csv\"\n",
    "elif vastai:\n",
    "    NEPTUNE_API_TOKEN = json.load(open(\"../apikeys.json\"))[\"NEPTUNE_API_TOKEN\"]\n",
    "    TRAIN_CSV = \"/root/lmsys/input/train.csv\"\n",
    "    FOLD_PATH = \"/root/lmsys/input/fold.pkl\"\n",
    "    # https://www.kaggle.com/code/rashmibanthia/lmsys-preprocess/output?scriptVersionId=189364525&select=lmsys_ext_df_hs.csv\n",
    "    EXTERNAL_CSV = \"/root/lmsys/input/external_datasets/lmsys_ext_df_hs.csv\"\n",
    "elif hf:\n",
    "    NEPTUNE_API_TOKEN = json.load(open(\"../apikeys.json\"))[\"NEPTUNE_API_TOKEN\"]\n",
    "    TRAIN_CSV = \"/data/lmsys/input/train.csv\"\n",
    "    FOLD_PATH = \"/data/lmsys/input/fold.pkl\"\n",
    "    login(token=json.load(open(\"../apikeys.json\"))[\"HF_TOKEN\"])\n",
    "    EXTERNAL_CSV = \"/data/lmsys/input/external_datasets/lmsys_ext_df_hs.csv\"\n",
    "else:\n",
    "    NEPTUNE_API_TOKEN = json.load(open(\"../../apikeys.json\"))[\"NEPTUNE_API_TOKEN\"]\n",
    "    TRAIN_CSV = \"/home/rashmi/Documents/kaggle/lmsys/input/train.csv\"\n",
    "    FOLD_PATH = \"/home/rashmi/Documents/kaggle/lmsys/input/fold.pkl\"\n",
    "\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = NEPTUNE_API_TOKEN\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"lmsys/lmsys\"\n",
    "os.environ[\"HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "# model_path =  \"RLHFlow/pair-preference-model-LLaMA3-8B\"\n",
    "# model_path = \"google/gemma-2-9b-it\"\n",
    "model_path = \"unsloth/gemma-2-9b-it-bnb-4bit\" \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# ## We have folds in External data\n",
    "# fold1_idx, val_idx = train[train.fold!=0].index , train[train.fold==0].index\n",
    "# print(len(fold1_idx), len(val_idx))\n",
    "\n",
    "## Incorrect labels\n",
    "list1 = train[(train['response_a'] == train['response_b']) & (train.winner_tie!=1) ]['id'].tolist()\n",
    "idx = train[train.id.isin(list1)].index\n",
    "train.loc[idx,'winner_model_a'] = 0\n",
    "train.loc[idx,'winner_model_b'] = 0\n",
    "train.loc[idx,'winner_tie'] = 1\n",
    "\n",
    "# train = train.head(100)\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57477, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "\n",
       "                                          response_b  label  \n",
       "0  [\"As an AI, I don't have personal beliefs or o...      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer and prepare dataset, metricsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60147521b7143cd92b91a8dd7ea4143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f861bbe9624b2b8bd163eb803c17bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c294192b55014ec48011fdfbb47318d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0918b814ab744e1aacb1c25182ff940a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:200]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:400]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:400]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ab261ec67449828d7379ca3fdc52d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(val_idx), len(fold1_idx):  5748 51729\n"
     ]
    }
   ],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    probs = softmax(logits, axis=-1)\n",
    "\n",
    "    ## save preds\n",
    "    np.save(f\"{OUTPUT_DIR}/preds_{fold_idx}.npy\", probs)\n",
    "\n",
    "    log_loss_ = log_loss(labels, probs)\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}\n",
    "\n",
    "\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "\n",
    "## Folds\n",
    "cv = joblib.load(FOLD_PATH)\n",
    "val_idx = cv[0][1] # fold 0\n",
    "fold1_idx = cv[0][0]\n",
    "print(\"len(val_idx), len(fold1_idx): \", len(val_idx), len(fold1_idx))\n",
    "train_idx, eval_idx = fold1_idx, val_idx\n",
    "\n",
    "if DEBUG:\n",
    "    train_idx, eval_idx = train_idx[:100], eval_idx[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Llama3ForSFT(Gemma2PreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Gemma2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        # if self.config.pretraining_tp > 1:\n",
    "        #     lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "        #     logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "        #     logits = torch.cat(logits, dim=-1)\n",
    "        # else:\n",
    "        \n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if self.config.final_logit_softcapping is not None:\n",
    "            logits = logits / self.config.final_logit_softcapping\n",
    "            logits = torch.tanh(logits)\n",
    "            logits = logits * self.config.final_logit_softcapping\n",
    "\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24729ec1bd3447ef86de75f50da926aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf26c4cce0ba4d2ba0fa3c86f5f71f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Llama3ForSFT were not initialized from the model checkpoint at unsloth/gemma-2-9b-it-bnb-4bit and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Llama3ForSFT(\n",
      "      (model): Gemma2Model(\n",
      "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-41): 42 x Gemma2DecoderLayer(\n",
      "            (self_attn): Gemma2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
      "              (rotary_emb): Gemma2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Gemma2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma2RMSNorm()\n",
      "            (post_attention_layernorm): Gemma2RMSNorm()\n",
      "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
      "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma2RMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 12,730,368 || all params: 10,171,940,352 || trainable%: 0.1252\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                )\n",
    "\n",
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16, \n",
    "    # quantization_config=quantization_config, \n",
    "    attn_implementation='eager'\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "# model.enable_input_require_grads()\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir = True,\n",
    "    eval_strategy = \"epoch\", #steps\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=2000,\n",
    "    # eval_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    optim=\"adamw_8bit\", #\"paged_adamw_32bit\", #\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    # bf16=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to=None if DEBUG  else \"neptune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e054051ffc804757a4334227c416436d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = ds.select(eval_idx)\n",
    "\n",
    "# Define a function to get the length of input_ids\n",
    "def get_input_ids_length(example):\n",
    "    return {\"input_length\": len(example[\"input_ids\"])}\n",
    "\n",
    "# Add a new column with the length of input_ids\n",
    "eval_dataset = eval_dataset.map(get_input_ids_length)\n",
    "\n",
    "# Sort the dataset by the length of input_ids in descending order\n",
    "eval_dataset = eval_dataset.sort(\"input_length\", reverse=True)\n",
    "\n",
    "# Remove the temporary input_length column\n",
    "eval_dataset = eval_dataset.remove_columns([\"input_length\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/lmsys/lmsys/e/LMSYS-180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3233' max='3233' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3233/3233 6:13:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.902794</td>\n",
       "      <td>0.577070</td>\n",
       "      <td>0.902794</td>\n",
       "      <td>557.750700</td>\n",
       "      <td>10.306000</td>\n",
       "      <td>2.576000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 13 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/lmsys/lmsys/e/LMSYS-180/metadata\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3233, training_loss=0.9701052817478292, metrics={'train_runtime': 22402.5965, 'train_samples_per_second': 2.309, 'train_steps_per_second': 0.144, 'total_flos': 2.399592130181923e+18, 'train_loss': 0.9701052817478292, 'epoch': 0.9999226784195469})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=eval_dataset, #.select(eval_idx),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_exp2/tokenizer/tokenizer_config.json',\n",
       " 'output_exp2/tokenizer/special_tokens_map.json',\n",
       " 'output_exp2/tokenizer/tokenizer.model',\n",
       " 'output_exp2/tokenizer/added_tokens.json',\n",
       " 'output_exp2/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create directory for tokenizer\n",
    "os.makedirs(f'{OUTPUT_DIR}/tokenizer', exist_ok=True)\n",
    "\n",
    "tokenizer.save_pretrained(f'{OUTPUT_DIR}/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748, 3)\n",
      "1.372473546096855\n"
     ]
    }
   ],
   "source": [
    "arr = np.load(f\"{OUTPUT_DIR}/preds_0.npy\")\n",
    "print(arr.shape)\n",
    "print(log_loss(train.loc[eval_idx, 'label'].values, arr) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.lm_head.state_dict(), f'{OUTPUT_DIR}/lm_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.save(\n",
    "# #     {\"model\": model.state_dict()},\n",
    "# #     f\"output/save_model/model_fold0_best.pth\",\n",
    "# # )\n",
    "model.model.save_pretrained(f'{OUTPUT_DIR}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5748\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.select(eval_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1437' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1437/1437 12:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/lmsys/lmsys/e/LMSYS-184\n"
     ]
    }
   ],
   "source": [
    "trainer_e = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = trainer_e.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9028096199035645,\n",
       " 'eval_accuracy': 0.5765483646485734,\n",
       " 'eval_log_loss': 0.902809530950303,\n",
       " 'eval_runtime': 769.2143,\n",
       " 'eval_samples_per_second': 7.473,\n",
       " 'eval_steps_per_second': 1.868}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748, 3)\n",
      "0.902809530950303\n"
     ]
    }
   ],
   "source": [
    "arr = np.load(f\"{OUTPUT_DIR}/preds_0.npy\")\n",
    "print(arr.shape)\n",
    "print(log_loss(train.loc[eval_idx, 'label'].values, arr) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
